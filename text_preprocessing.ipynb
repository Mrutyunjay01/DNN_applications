{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_preprocessing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN4uNdD28Q9JK6IEGJCYthI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mrutyunjay01/DNN_applications/blob/master/text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcqZx2NO6b50",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXBAz8cI4hHz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4a6b98f9-61ed-4ba4-e571-9f8678b1a40b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukNFXTk55y32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = 'Hey, I love You!'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50ac2QBx6r-S",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   nltk.tokenize.WhitespaceTokenizer() separates words with spaces\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHJ6RFd957ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERor-WfM6GFr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6ae69ba3-2075-4633-a7f3-ec8a0d71ee76"
      },
      "source": [
        "tokens"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hey,', 'I', 'love', 'You!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f2Y3_Rs7SvV",
        "colab_type": "text"
      },
      "source": [
        " * WordPunctTokenizer() separates using punctuation marks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtRjhVph67cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
        "tokens = tokenizer.tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlf2QTeR7kha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "90bc3447-e5ce-4539-b8fb-11514800fef0"
      },
      "source": [
        "tokens"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hey', ',', 'I', 'love', 'You', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln9JNOp67vH7",
        "colab_type": "text"
      },
      "source": [
        "* TreebankWordTokenizer() separates words on basis of their morphological meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye_IeWK67mJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
        "tokens = tokenizer.tokenize(\"Hey!, What's up? _Buddy?\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHOH2L1C7_IK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c32ee28f-eb6e-45d5-e267-bdf5f5cf5072"
      },
      "source": [
        "tokens"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hey', '!', ',', 'What', \"'s\", 'up', '?', '_Buddy', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0YNaMmQ8VUJ",
        "colab_type": "text"
      },
      "source": [
        "# Token Normalization\n",
        " * Stemming\n",
        "  - heuristics that chop off suffixes\n",
        " * Lemmatization\n",
        "  - Returns the base or dictionary form of a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FFa4li_-emO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = \"feet cats wolves talkes\"\n",
        "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
        "tokens = tokenizer.tokenize(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4qrQpQ--1-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d44bdcbd-9e4b-4e7b-b9e6-363c3fe70ba4"
      },
      "source": [
        "# Stemming example\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "print(words)\n",
        "\" \".join(stemmer.stem(token) for token in tokens)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feet cats wolves talkes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'feet cat wolv talk'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9iMAuGi_Zgj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "40b059bf-8718-44a4-8454-9c535b5f617f"
      },
      "source": [
        "# Lemmatization example\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "print(words)\n",
        "\" \".join(lemmatizer.lemmatize(token) for token in tokens)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feet cats wolves talkes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'foot cat wolf talkes'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "villlQUlBOg6",
        "colab_type": "text"
      },
      "source": [
        "# Other types of normalization\n",
        "  * Normalizing capital letters\n",
        "    - lowercasing the beginning of the sentence\n",
        "    - lowercasing words in titles\n",
        "    - leave mid sentence words as they are\n",
        "  * Acronyms\n",
        "    - e.t.a, E.T.A as E.T.A\n",
        "    - or use regular expression for identifying such words for every probable combination( Hard!!!)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gajl0cK5_xve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lish6hQJB92Y",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "  - We can think of a text as sequences \n",
        "  - Tokenization is a process of extracting those tokens\n",
        "  - we can normalize the tokens using stemming or lemmatization\n",
        "  - casing and acronyms can also be normalised."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiH2-nr4CXBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}